{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples below are adapted from [\"How To Work with Language Data in Python 3 using the Natural Language Toolkit (NLTK)\"](https://www.digitalocean.com/community/tutorials/how-to-work-with-language-data-in-python-3-using-the-natural-language-toolkit-nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.__version__) \n",
    "# You should have version 3.2.1 installed\n",
    "# since we'll use NLTK's Twitter package that requires this version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_samples # has a specific type: TwitterCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's twitter corpus currently contains a sample of 20,000 tweets retrieved from the Twitter Streaming API. Full tweets are stored as line-separated JSON. We can see how many JSON files exist in the corpus using the `twitter_samples.fileids()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those file IDs we can then return the tweet strings. \n",
    "Let's look at just the first few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_samples.strings('tweets.20150430-223406.json')[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know our corpus was downloaded successefully, so we can start processing the tweets.\n",
    "\n",
    "Let's count how many adjectives and nouns appear in the positive subset of the `twitter_samples` corpus:\n",
    "\n",
    "* A **noun**, in its most basic definition, is usually defined as a person, place, or thing (e.g., a movie, a book, and a burger). Counting nouns can help determine how many different _topics_ are being discussed.\n",
    "* An **adjective** is a word that modifies a noun (or pronoun), for example: a _horrible_ movie, a _funny_ book, or a _delicious_ burger. Counting adjectives can determine what type of language is being used, i.e. opinions tend to include more adjectives than facts.\n",
    "\n",
    "We could later count positive adjectives (great, awesome, happy, etc.) versus negative adjectives (boring, lame, sad, etc.), which could be used to analyze the sentiment of tweets or reviews about a product or movie, for example. This script provides data that can in turn inform decisions related to that product or movie.\n",
    "\n",
    "\n",
    "Let's create a `tweets` variable and assign to it the list of tweet strings from the `positive_tweets.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = twitter_samples.strings('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Sentences\n",
    "\n",
    "**Tokenization** is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements, which are called _tokens_. Let's create a new variable called `tweets_tokens`, to which we will assign the tokenized list of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized tweets\n",
    "tweets_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new variable, `tweets_tokens`, is a list where each element in the list is a list of tokens. \n",
    "\n",
    "We can compare the list of tokens to the original tweet that the tokens came from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets[0:1])\n",
    "tweets_tokens[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tokens of each tweet we can tag the tokens with the appropriate parts of speech (POS) tags.\n",
    "\n",
    "\n",
    "In order to access NLTK's POS tagger, we'll need to import it. (Typically, all import statements must go at the beginning of the script/notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag_sents\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can tag each of our tokens. NLTK allows us to do it all at once using: `pos_tag_sents()`. We are going to create a new variable `tweets_tagged`, which we will use to store our tagged lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tagged = pos_tag_sents(tweets_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what tagged tokens look like, here is what the first element in our tweets_tagged list looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tagged[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging parts of speech (POS)\n",
    "\n",
    "We can see that our tweet is represented as a list and for each token we have information about its POS tag. Each token/tag pair is saved as a [tuple](https://www.digitalocean.com/community/tutorials/understanding-tuples-in-python-3). The default tagger of `nltk.pos_tag()` uses the Penn Treebank Tag Set. You can check an [alphabetical list of part-of-speech tags used in the Penn Treebank Project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "In NLTK, the abbreviation for **adjective** is `JJ`.\n",
    "\n",
    "The NLTK tagger marks **singular nouns** (`NN`) with different tags than **plural nouns** (`NNS`). To simplify, we will only count singular nouns by keeping track of the `NN` tag.\n",
    "\n",
    "In the next step we will count how many times `JJ` and `NN` appear throughout our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep track of how many times JJ and NN appear using an accumulator (count) variable, which we will continuously add to every time we find a tag. \n",
    "\n",
    "After we create the variables, we'll create two for loops. The first loop will iterate through each tweet in the list. The second loop will iterate through each token/tag pair in each tweet. For each pair, we will look up the tag using the appropriate tuple index.\n",
    "\n",
    "We will then check to see if the tag matches either the string 'JJ' or 'NN' by using conditional statements. If the tag is a match we will add (+= 1) to the appropriate accumulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set accumulators\n",
    "JJ_count = 0\n",
    "NN_count = 0\n",
    "\n",
    "# Loop through list of tweets\n",
    "for tweet in tweets_tagged:\n",
    "    for pair in tweet:\n",
    "        tag = pair[1]\n",
    "        if tag == 'JJ':\n",
    "            JJ_count += 1\n",
    "        elif tag == 'NN':\n",
    "            NN_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the two loops are complete, we should have the total count for adjectives and nouns in our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of adjectives = ', JJ_count)\n",
    "print('Total number of nouns = ', NN_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
