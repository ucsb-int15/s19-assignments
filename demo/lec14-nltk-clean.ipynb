{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful links and resources\n",
    "\n",
    "* [Everything you need to know about natural language processing](https://blog.algorithmia.com/introduction-natural-language-processing-nlp/)\n",
    "\n",
    "* NLTK Book here: http://www.nltk.org/nltk_data/\n",
    "\n",
    "* [Tracking Progress in Natural Language Processing](https://nlpprogress.com):  tracks the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.\n",
    "\n",
    "* Some examples in this notebook are from  https://github.com/edbullen/nltk\n",
    "\n",
    "* Youtube link: [Natural Language Processing (NLP) & Text Mining Tutorial Using NLTK | Edureka](https://www.youtube.com/watch?v=05ONoGfmKvA)\n",
    "\n",
    "\n",
    "# NLTK Corpora and Treebanks\n",
    "\n",
    "In the field of Natural Langugage Processing, a sample of real world text is referred to as a *Corpus* (plural *Corpora*).\n",
    "\n",
    "Possibly the most famous and widely used corpus is the **Brown Corpus** (https://en.wikipedia.org/wiki/Brown_Corpus) which was the first million-word electronic corpus of English, created in 1961 at Brown University.\n",
    "\n",
    "A *Treebank* is a parsed and annotated Corpus that records the syntactic or semantic sentence structure of the content (i.e. typically each sentence and probably each word in the corpus).   The first large-scale treebank was The **Penn Treebank**, created in 1992 at the University of Pennsylvania. \n",
    "\n",
    "There are many corpora and treebanks distributed with the NLTK toolkit, listed in the NLTK Book here: http://www.nltk.org/nltk_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various methods can be applied against a loaded corpus to perform actions such as extract the words from corpus, identify the files that make up a corpus, list category definitions in a corpus and extract text filtered by category.\n",
    "\n",
    "A list of common methods is provided in Chapter 2 of the [NLTK Book](http://www.nltk.org/book/ch02.html) as follows:  \n",
    "<table align=\"left\">\n",
    "<tr><td><b>Example</b></td>   <td align=\"left\"><b>Description</b></td></tr>\n",
    "<tr><td>fileids()</td>   <td align=\"left\">the files of the corpus</td></tr>\n",
    "<tr><td>fileids([categories])</td>   <td align=\"left\">the files of the corpus corresponding to these categories</td></tr>\n",
    "<tr><td>categories()</td>   <td align=\"left\">the categories of the corpus</td></tr>\n",
    "<tr><td>categories([fileids])</td>   <td align=\"left\">the categories of the corpus corresponding to these files</td></tr>\n",
    "<tr><td>raw()</td>   <td align=\"left\">the raw content of the corpus</td></tr>\n",
    "<tr><td>raw(fileids=[f1,f2,f3])</td>   <td align=\"left\">the raw content of the specified files</td></tr>\n",
    "<tr><td>raw(categories=[c1,c2])</td>   <td align=\"left\">the raw content of the specified categories</td></tr>\n",
    "<tr><td>words()</td>   <td align=\"left\">the words of the whole corpus</td></tr>\n",
    "<tr><td>words(fileids=[f1,f2,f3])</td>   <td align=\"left\">the words of the specified fileids</td></tr>\n",
    "<tr><td>words(categories=[c1,c2])</td>   <td align=\"left\">the words of the specified categories</td></tr>\n",
    "<tr><td>sents()</td>   <td align=\"left\">the sentences of the whole corpus</td></tr>\n",
    "<tr><td>sents(fileids=[f1,f2,f3])</td>   <td align=\"left\">the sentences of the specified fileids</td></tr>\n",
    "<tr><td>sents(categories=[c1,c2])</td>   <td align=\"left\">the sentences of the specified categories</td></tr>\n",
    "<tr><td>abspath(fileid)</td>   <td align=\"left\">the location of the given file on disk</td></tr>\n",
    "<tr><td>encoding(fileid)</td>   <td align=\"left\">the encoding of the file (if known)</td></tr>\n",
    "<tr><td>open(fileid)</td>   <td align=\"left\">open a stream for reading the given corpus file</td></tr>\n",
    "<tr><td>root</td>   <td align=\"left\">if the path to the root of locally installed corpus</td></tr>\n",
    "<tr><td>readme()</td>   <td align=\"left\">the contents of the README file of the corpus</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "print(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [fileid[:4] for fileid in inaugural.fileids()] # get the first 4 characters of the fileid\n",
    "print(years[0], years[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inaugural.words('1789-Washington.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inaugural.sents('1789-Washington.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inaugural.words('2009-Obama.txt')))\n",
    "print(len(inaugural.sents('2009-Obama.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inaugural.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the frequency of the use of some words changes\n",
    "# example from https://stackoverflow.com/questions/51414876/python-nltk-inaugural-text-corpora-hands-on-solution-needed\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    [(fileid[:4], target) \n",
    "     for fileid in inaugural.fileids() \n",
    "     for w in inaugural.words(fileid) \n",
    "     for target in ['america', 'citizen'] \n",
    "     if w.lower().startswith(target)])\n",
    "\n",
    "cfd.tabulate(conditions=['1841', '1993', '2009'], \n",
    "             samples=['america', 'citizen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from https://www.nltk.org/book/ch02.html\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    [(target, fileid[:4]) \n",
    "     for fileid in inaugural.fileids() \n",
    "     for w in inaugural.words(fileid) \n",
    "     for target in ['america', 'citizen'] \n",
    "     if w.lower().startswith(target)])\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "tokens = word_tokenize(input_str)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing (NLP) is a field \" + \\\n",
    "       \"of computer science, artificial intelligence \" + \\\n",
    "       \"and computational linguistics concerned with \" + \\\n",
    "       \"the interactions between computers and human \" + \\\n",
    "       \"(natural) languages, and, in particular, \" + \\\n",
    "       \"concerned with programming computers to \" + \\\n",
    "       \"fruitfully process large natural language \" + \\\n",
    "       \"corpora. Challenges in natural language \" + \\\n",
    "       \"processing frequently involve natural \" + \\\n",
    "       \"language understanding, natural language\" + \\\n",
    "       \"generation frequently from formal, machine\" + \\\n",
    "       \"-readable logical forms), connecting language \" + \\\n",
    "       \"and machine perception, managing human-\" + \\\n",
    "       \"computer dialog systems, or some combination \" + \\\n",
    "       \"thereof.\"\n",
    "text_sent = sent_tokenize(text)\n",
    "text_tokens = word_tokenize(text)\n",
    "print(text_sent) \n",
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_str)\n",
    "\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "text_stem = []\n",
    "for token in text_tokens:\n",
    "    text_stem.append(stemmer.stem(token))\n",
    "#     print(stemmer.stem(token))\n",
    "print(text_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result doesn't match what's here: \n",
    "# https://www.tutorialspoint.com/python/python_synonyms_and_antonyms.htm\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"Soil\"):\n",
    "    for lm in syn.lemmas():\n",
    "             synonyms.append(lm.name())\n",
    "print (set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"ahead\"):\n",
    "    for lm in syn.lemmas():\n",
    "        if lm.antonyms():\n",
    "            antonyms.append(lm.antonyms()[0].name())\n",
    "\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets See how the movies are classified\n",
    "# https://www.tutorialspoint.com/python/python_text_classification.htm\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = movie_reviews.fileids()\n",
    "#print(fields)\n",
    "fields[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_reviews.raw(\"neg/cv000_29416.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(all_words) # get the frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of samples\n",
    "fdist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a word's frequency\n",
    "print (fdist.freq('teen'))\n",
    "print (fdist.freq('looooot'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Hapax_legomenon\n",
    "# In corpus linguistics, a hapax legomenon \n",
    "# sometimes abbreviated to hapax is a word that occurs only once \n",
    "# within a context, either in the written record of an entire language, \n",
    "# in the works of an author, or in a single text.\n",
    "\n",
    "fdist.hapaxes()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Draw a bar chart with the count of the most common words\n",
    "# import matplotlib.pyplot as plt\n",
    "x, y = zip(*fdist.most_common(n=20)) # Unzip the tuples into lists\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.bar(range(len(x)), y)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_clean = [i for i in all_words if not i in stop_words]\n",
    "fdist_clean = nltk.FreqDist(all_words_clean) # get the frequency distribution\n",
    "print(fdist_clean.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Draw a bar chart with the count of the most common words\n",
    "import matplotlib.pyplot as plt\n",
    "x, y = zip(*fdist_clean.most_common(n=20)) # Unzip the tuples into lists\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.bar(range(len(x)), y)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_clean2 = [i for i in all_words_clean if not i in string.punctuation]\n",
    "fdist_clean2 = nltk.FreqDist(all_words_clean2) # get the frequency distribution\n",
    "print(fdist_clean2.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"film\"):\n",
    "    for lm in syn.lemmas():\n",
    "             synonyms.append(lm.name())\n",
    "print (set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw a bar chart with the count of the most common words\n",
    "import matplotlib.pyplot as plt\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'regular',\n",
    "        'size'   : 22}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "x, y = zip(*fdist_clean2.most_common(n=15)) # Unzip the tuples into lists\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.bar(range(len(x)), y)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text\n",
    "from nltk.corpus import reuters\n",
    " \n",
    "text = Text(reuters.words())\n",
    "# via https://nlpforhackers.io/introduction-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a specific category\n",
    "\n",
    "# Example from https://towardsdatascience.com/@badreeshshetty\n",
    "# Get the trade words\n",
    "tradeWords = reuters.words(categories = 'trade')\n",
    "len(tradeWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation\n",
    "# This takes a couple of minutes to run\n",
    "tradeWords = [w for w in tradeWords if w.lower() not in stopwords.words('english') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeWords = [w for w in tradeWords if w not in string.punctuation]\n",
    "punctCombo = [c+\"\\\"\" for c in string.punctuation ]+ [\"\\\"\"+c for c in string.punctuation ]\n",
    "tradeWords = [w for w in tradeWords if w not in punctCombo]\n",
    "len(tradeWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(tradeWords)\n",
    "fdist.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biTradeWords = nltk.bigrams(tradeWords)\n",
    "biFdist = nltk.FreqDist(biTradeWords)\n",
    "biFdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biFdist.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words that appear in similar contexts\n",
    "text.similar('Monday', 5) \n",
    "# april march friday february january\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get common contexts for a list of words\n",
    "text.common_contexts(['August', 'June']) \n",
    "# since_a in_because last_when between_and last_that and_at ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get contexts for a word\n",
    "text.concordance('Monday')\n",
    "# said . Trade Minister Saleh said on Monday that Indonesia , as the world ' s s\n",
    "# Reuters to clarify his statement on Monday in which he said the pact should be\n",
    "#  the 11 - member CPA which began on Monday . They said producers agreed that c\n",
    "# ief Burkhard Junger was arrested on Monday on suspicion of embezzlement and of\n",
    "# ween one and 1 . 25 billion dlrs on Monday and Tuesday . The spokesman said Mo\n",
    "# ay and Tuesday . The spokesman said Monday ' s float included 500 mln dlrs in \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA\n",
    "Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual-usage meaning of words by statistical computations applied to a large corpus of text.\n",
    "\n",
    "LSA is an information retrieval technique which analyzes and identifies the pattern in unstructured collection of text and the relationship between them.\n",
    "\n",
    "LSA is an unsupervised way of uncovering synonyms in a collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TFIDF is an information retrieval technique that weighs a term's frequency (TF) and its inverse document frequency (IDF). Each word has its respective TF and IDF score. The product of the TF and IDF scores of a word is called the TFIDF weight of that word.\n",
    "\n",
    "Put simply, the higher the TFIDF score (weight), the rarer the word and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
