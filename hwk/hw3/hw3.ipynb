{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('hw3.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Homework 3: Trump, Twitter, and Movies\n",
    "\n",
    "## Due Date: Friday, May 31 5:00 pm PST\n",
    "\n",
    "Welcome to the third homework assignment of INT15! In this assignment, we will work with Twitter data in order to analyze Donald Trump's tweets.\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the homework, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list collaborators here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "# Ensure that Pandas shows at least 280 characters in columns, so we can see full tweets\n",
    "pd.set_option('max_colwidth', 280)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to unzip and read tweets from the json file into a list named `all_tweets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the dataset\n",
    "dest_path = 'hw3-realdonaldtrump_tweets.json.zip'\n",
    "my_zip = zipfile.ZipFile(dest_path, 'r')\n",
    "with my_zip.open('hw3-realdonaldtrump_tweets.json', 'r') as f:\n",
    "    all_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "formatting-note",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Here is what a typical tweet from `all_tweets` looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "pprint-old-tweets",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint # to get a more easily-readable view.\n",
    "pprint(all_tweets[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Let's construct a DataFrame called `trump` containing data from all the tweets stored in `all_tweets`. The index of the DataFrame should be the ID of each tweet (looks something like `907698529606541312`). It should have these columns:\n",
    "\n",
    "- `time`: The time the tweet was created encoded as a datetime object. (Use `pd.to_datetime` to encode the timestamp.)\n",
    "- `source`: The source device of the tweet.\n",
    "- `text`: The text of the tweet.\n",
    "- `retweet_count`: The retweet count of the tweet. \n",
    "\n",
    "Finally, **the resulting DataFrame should be sorted by the index.**\n",
    "\n",
    "**Warning:** *Some tweets will store the text in the `text` field and other will use the `full_text` field.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "scrolled": true,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "trump = pd.DataFrame({\n",
    "    'time': pd.to_datetime([tweet['created_at'] for tweet in all_tweets]).tz_convert(None),\n",
    "    'source': [tweet['source'] for tweet in all_tweets],\n",
    "    'text': [tweet['text'] if \"text\" in tweet else tweet['full_text'] for tweet in all_tweets],\n",
    "    'retweet_count': \n",
    "}, index=[tweet['id'] for tweet in all_tweets],\n",
    "   columns=['time', 'source', 'text', 'retweet_count'],\n",
    ").sort_index()\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "question4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Part 2: Tweet Source Analysis\n",
    "\n",
    "In the following questions, we are going to find out the charateristics of Trump tweets and the devices used for the tweets.\n",
    "\n",
    "First let's examine the `'source'` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "unique-sources",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trump['source'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "Notice how sources like \"Twitter for Android\" or \"Instagram\" are surrounded by HTML tags. In the cell below, clean up the `source` field by removing the HTML tags from each `source` entry.\n",
    "\n",
    "**Hints:** \n",
    "* Use `trump['source'].str.replace` along with a regular expression.\n",
    "* You may find it helpful to experiment with regular expressions at [regex101.com](https://regex101.com/).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Uncomment and complete\n",
    "# trump['source'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "note-about-device-usage",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In the following plot, we see that there are two device types that are more commonly used than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "device-usage-plot",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "trump['source'].value_counts().plot(kind=\"bar\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.title(\"Number of Tweets by Source\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Now that we have cleaned up the `source` field, let's now look at which device Trump has used over the entire time period of this dataset.\n",
    "\n",
    "To examine the distribution of dates we will convert the date to a fractional year that can be plotted as a distribution.\n",
    "\n",
    "(Code borrowed from https://stackoverflow.com/questions/6451655/python-how-to-convert-datetime-dates-to-decimal-years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "fractional-year",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def year_fraction(date):\n",
    "    start = datetime.date(date.year, 1, 1).toordinal()\n",
    "    year_length = datetime.date(date.year+1, 1, 1).toordinal() - start\n",
    "    return date.year + float(date.toordinal() - start) / year_length\n",
    "\n",
    "trump['year'] = trump['time'].apply(year_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q5a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now, use `sns.distplot` to overlay the distributions of Trump's 2 most frequently used web technologies over the years. Your final plot should look like:\n",
    "\n",
    "<img src=\"images/source_years_q3.png\" width=\"600px\" />\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q5a-answer",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hint: use sns.distplot(..., label = ...)\n",
    "top_devices = ...\n",
    "for device in top_devices:\n",
    "    ...\n",
    "plt.title('Distributions of Tweet Sources Over Years')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 4\n",
    "\n",
    "\n",
    "Is there a difference between Trump's tweet behavior across these devices? We will attempt to answer this question in our subsequent analysis.\n",
    "\n",
    "First, we'll take a look at whether Trump's tweets from an Android device come at different times than his tweets from an iPhone. Note that Twitter gives us his tweets in the [UTC timezone](https://www.wikiwand.com/en/List_of_UTC_time_offsets) (notice the `+0000` in the first few tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "tweet-created-at",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for tweet in all_tweets[:3]:\n",
    "    print(tweet['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "convert-to-est-justification",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We'll convert the tweet times to US Eastern Time, the timezone of New York and Washington D.C., since those are the places we would expect the most tweet activity from Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "convert-to-est",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trump['est_time'] = (\n",
    "    trump['time'].dt.tz_localize(\"UTC\") # Set initial timezone to UTC\n",
    "                 .dt.tz_convert(\"EST\") # Convert to Eastern Time\n",
    ")\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "need-to-do",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 4a\n",
    "\n",
    "Add a column called `hour` to the `trump` table which contains the hour of the day as a floating point number computed by:\n",
    "\n",
    "$$\n",
    "\\text{hour} + \\frac{\\text{minute}}{60} + \\frac{\\text{second}}{60^2}\n",
    "$$\n",
    "\n",
    "* **Hint:** See the cell above for an example of working with [dt accessors](https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html#basics-dt-accessors). E.g., use `dt.hour` to get the hour from the `trump['est_time']` series.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4a\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "trump['hour'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q4a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 4b\n",
    "\n",
    "Use this data along with the seaborn `distplot` function to examine the distribution over hours of the day in eastern time that trump tweets on each device for the 2 most commonly used devices.  Your plot should look similar to the following:\n",
    "\n",
    "<img src=\"images/device_hour4b.png\" width=\"600px\" />\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4b\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4c-answer",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### make your plot here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 4c\n",
    "\n",
    "According to [this Verge article](https://www.theverge.com/2017/3/29/15103504/donald-trump-iphone-using-switched-android), Donald Trump switched from an Android to an iPhone sometime in March 2017.\n",
    "\n",
    "Let's see if this information significantly changes our plot. Create a figure similar to your figure from question 4b, but this time, only use tweets that were tweeted before 2017. Your plot should look similar to the following:\n",
    "\n",
    "<img src=\"images/device_hour4c.png\" width=\"600px\" />\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4c\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9d60149ec24272e3",
     "locked": false,
     "points": 0,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### make your plot here \n",
    "### remember that you can add a condition like so: trump[trump['year'] < ...]\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4d\n",
    "\n",
    "During the campaign, it was theorized that Donald Trump's tweets from Android devices were written by him personally, and the tweets from iPhones were from his staff. Does your figure give support to this theory? What kinds of additional analysis could help support or reject this claim?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4d\n",
    "points: 1\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "written"
    ]
   },
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Part 3: Sentiment Analysis\n",
    "\n",
    "It turns out that we can use the words in Trump's tweets to calculate a measure of the sentiment of the tweet. For example, the sentence \"I love America!\" has positive sentiment, whereas the sentence \"I hate taxes!\" has a negative sentiment. In addition, some words have stronger positive / negative sentiment than others: \"I love America.\" is more positive than \"I like America.\"\n",
    "\n",
    "We will use the [VADER (Valence Aware Dictionary and sEntiment Reasoner)](https://github.com/cjhutto/vaderSentiment) lexicon to analyze the sentiment of Trump's tweets. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media which is great for our usage.\n",
    "\n",
    "The VADER lexicon gives the sentiment of individual words. Run the following cell to show the first few rows of the lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "head-vader",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(''.join(open(\"vader_lexicon.txt\").readlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q6a-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 5\n",
    "\n",
    "As you can see, the lexicon contains emojis too! Each row contains a word and the *polarity* of that word, measuring how positive or negative the word is.\n",
    "\n",
    "(How did they decide the polarities of these words? What are the other two columns in the lexicon? See the link above.)\n",
    "\n",
    "### Question 5a\n",
    "\n",
    "Read in the lexicon into a DataFrame called `sent`. The index of the DataFrame should be the words in the lexicon. `sent` should have one column named `polarity`, storing the polarity of each word.\n",
    "\n",
    "* **Hint:** The `pd.read_csv` function may help here. Note that `pd.read_csv` can read data in other, similar formats, especially, if its parameter `sep` is set correctly. You might also look up which parameters will allow you to select specific columns and set an index column.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5a\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6a1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "sent = ...\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q5a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q6b-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5b\n",
    "\n",
    "Now, let's use this lexicon to calculate the overall sentiment for each of Trump's tweets. Here's the basic idea:\n",
    "\n",
    "1. For each tweet, find the sentiment of each word.\n",
    "2. Calculate the sentiment of each tweet by taking the sum of the sentiments of its words.\n",
    "\n",
    "First, let's lowercase the text in the tweets since the lexicon is also lowercase. Set the `text` column of the `trump` DataFrame to be the lowercased text of each tweet.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5b\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q5b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q6c-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5c\n",
    "\n",
    "Now, let's get rid of punctuation since it will cause us to fail to match words. Create a new column called `no_punc` in the `trump` DataFrame to be the lowercased text of each tweet with all punctuation replaced by a single space. We consider punctuation characters to be *any character that isn't a Unicode word character or a whitespace character* (consult the Python documentation on regexes for how to represent them).\n",
    "\n",
    "(Why don't we simply remove punctuation instead of replacing with a space? See if you can figure this out by looking at the tweet data.)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5c\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6c",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Save your regex in punct_re\n",
    "punct_re = r''\n",
    "trump['no_punc'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q5c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q6d-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5d\n",
    "\n",
    "Now, let's convert the tweets into what's called a [*tidy format*](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) to make the sentiments easier to calculate. Use the `no_punc` column of `trump` to create a table called `tidy_format`. **The index of the table should be the IDs of the tweets, repeated _once for every word_ in the tweet**. It has two columns:\n",
    "\n",
    "1. `num`: The location of the word in the tweet. For example, if the tweet was \"i love america\", then the location of the word \"i\" is 0, \"love\" is 1, and \"america\" is 2.\n",
    "2. `word`: The individual words of each tweet.\n",
    "\n",
    "The first few rows of our `tidy_format` table look like:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>num</th>\n",
    "      <th>word</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>0</td>\n",
    "      <td>i</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>1</td>\n",
    "      <td>think</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>2</td>\n",
    "      <td>senator</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>3</td>\n",
    "      <td>blumenthal</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>4</td>\n",
    "      <td>should</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "**Note that your DataFrame may look different from the one above.** However, you can double check that your tweet with ID `894661651760377856` has the same rows as ours. Our tests don't check whether your table looks exactly like ours.\n",
    "\n",
    "As usual, try to **avoid using** any `for` loops. Our solution uses a chain of 5 methods on the `trump` DataFrame, albeit using some rather advanced Pandas hacking.\n",
    "\n",
    "* **Hint 1:** Try looking at the `expand` argument to pandas' `str.split`.\n",
    "\n",
    "* **Hint 2:** Try looking at the `stack()` method.\n",
    "\n",
    "* **Hint 3:** Try looking at the `level` parameter of the `reset_index` method.\n",
    "\n",
    "* **Hint 4:** You might need to `rename(columns={...})`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5d\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "tidy_format = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q5d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q6e-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5e\n",
    "\n",
    "Now that we have this table in the tidy format, it becomes much easier to find the sentiment of each tweet: we can join the table with the lexicon table. \n",
    "\n",
    "Add a `polarity` column to the `trump` table.  The `polarity` column should contain **the sum of the sentiment polarity** of each word in the text of the tweet.\n",
    "\n",
    "**Hints:** \n",
    "* You will need to `merge` the `tidy_format` and `sent` tables and group the final answer on the `index`.\n",
    "* If certain words are not found in the `sent` table, set their polarities to 0 (use `fillna()`).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5e\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6e",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "trump['polarity'] = (\n",
    "    tidy_format\n",
    "    .merge(sent, how='left', left_on='word', right_index=True)\n",
    "    .reset_index()\n",
    ")\n",
    "trump[['text', 'polarity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q5e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "a-note-on-vader",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now we have a measure of the sentiment of each of his tweets! Note that this calculation is rather basic; you can read over the VADER readme to understand a more robust sentiment analysis.\n",
    "\n",
    "Now, run the cells below to see the most positive and most negative tweets from Trump in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "negative-tweets",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print('Most negative tweets:')\n",
    "for t in trump.sort_values('polarity').head()['text']:\n",
    "    print('\\n  ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "postive-tweets",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Most positive tweets:')\n",
    "for t in trump.sort_values('polarity', ascending=False).head()['text']:\n",
    "    print('\\n  ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q6g",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 6\n",
    "\n",
    "Now, let's try looking at the distributions of sentiments for tweets containing certain keywords.\n",
    "\n",
    "### Question 6a\n",
    "\n",
    "In the cell below, create a single plot showing both the distribution of tweet sentiments for tweets containing `nytimes`, as well as the distribution of tweet sentiments for tweets containing `fox`.\n",
    "\n",
    "**Hint**: You can use `str.contains()`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6a\n",
    "points: 1\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q6g-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "plt.title('Distributions of Tweet Polarities (nytimes vs. fox)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "comment-on-faux-news",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 6b\n",
    "Comment on what you observe in the plot above. Can you find other pairs of keywords that lead to interesting plots? (If you modify your code in 6a, remember to change the words back to `nytimes` and `fox` before submitting for grading).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6b\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q6g-written",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Part 4: Movie Recommendation System\n",
    "\n",
    "## Question 7\n",
    "\n",
    "Your TA, Pushkar, has only recently started watching movies. While chatting with you, he asks you to recommed some good movies to add to his queue. Normally, you would have told him the movies that you liked but since you are taking INT15, you decide to do this in a data-science fashion. You create your own movie recommendation system that can later be used by Pushkar to watch the movies that he might like.\n",
    "\n",
    "\n",
    "### Question 7a: Data\n",
    "\n",
    "You decide to use the IMDB dataset [https://www.imdb.com/interfaces](https://www.imdb.com/interfaces/), which contains the names of the movies along with their respective ratings. \n",
    "\n",
    "Let's import the top 1000 movies with their plots stored in the files `top_1000_movies.csv` and `plots.csv`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7a\n",
    "points: 1\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies = ...\n",
    "plots = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q7a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview the top entries of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that we do not have any TV series, and if that is true, remove (`drop`) the \"titleType\" and \"endYear\" columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_movies[all_movies.titleType != \"movie\"] # should return an empty dataframe if we have only movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the uninformative columns mentioned above\n",
    "movies = ...\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check, how many movies have their primary title differ from their original title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_titles = ...\n",
    "len(diff_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 7b: Merge the datasets\n",
    "As you can see, both dataframes have a common ID (although, the column names do not match). Let's join the two files to combine the movies with their plots. Use the `df.merge` command in Pandas.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.rename(columns={'Unnamed: 0':'tconst'}, inplace=True)\n",
    "# Write your code to merge the two dataframes\n",
    "movies = ...\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q7b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7c: Text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's preprocess the plot of each movie by:\n",
    "\n",
    "1. Converting all words to lower case\n",
    "\n",
    "2. Removing numbers and punctuation\n",
    "\n",
    "3. Tokenizing\n",
    "    \n",
    "4. Removing stop words \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['key_words'] = \"\" # add a new column\n",
    "movies['key_words'] = np.nan\n",
    "\n",
    "# Convert to lowercase and remove leading/trailing whitespace\n",
    "movies['key_words'] = (movies['plots']\n",
    " .str.lower()\n",
    " .str.strip()\n",
    ")\n",
    "\n",
    "# Remove Punctuation\n",
    "punct_regex = ...\n",
    "movies['key_words'] = movies['key_words'].str.replace(punct_regex, ' ')\n",
    "\n",
    "# Remove Numbers\n",
    "num_regex = ...\n",
    "...\n",
    "\n",
    "# Extract the keywords (exclude stop words)\n",
    "movies['key_words'] = (movies['key_words']\n",
    "                       .apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words])))\n",
    "\n",
    "# Tokenize the plots using word_tokenize  (use .apply like above)\n",
    "...\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q7c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a set of keywords for each of the movies. We can use these keywords to create a dictionary and use it to find movies with similar keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 7d\n",
    "\n",
    "For analyzing each word in a corpus, we associate that word with a number by creating a dictionary. The dicitionary can be created using the `Dictionary` module from `gensim`. \n",
    " \n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7d\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Keywords\n",
    "processed_keywords=[]\n",
    "for keywords in movies['key_words']:\n",
    "    processed_keywords.append(keywords)\n",
    "    \n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# Write your code here\n",
    "dictionary = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q7d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the frequency distribution of the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### concatenate all keywords from all movies together\n",
    "all_movie_words=[]\n",
    "for word in processed_keywords:\n",
    "    all_movie_words.extend(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freq_words(words, terms = 10):\n",
    "\n",
    "    fdist = FreqDist(words)\n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n",
    "\n",
    "    # selecting top most frequent words\n",
    "    d = words_df.nlargest(columns=\"count\", n = terms) \n",
    "    plt.figure(figsize=(20,5))\n",
    "    ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n",
    "    ax.set(ylabel = 'Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq_words(all_movie_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you that the figure represents the top words and their frequencies in the corpus. Now we can do this for the  keywords for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq_words(processed_keywords[70])  # get the frequency distribution for the words from a specific movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess which movie we selected above just by looking at the keywords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 7e\n",
    "\n",
    "What we plotted above is known as a _bag of words_.\n",
    "The Bag of Words is a list of tuples where the first index denotes the postion of the word in the dictionary while the second index denotes it's frequency. \n",
    "\n",
    "Go through all keywords in `processed_keywords` and add them to the dictionary we created above using the `doc2bow` command.\n",
    " \n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7e\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q7e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7f: TF-IDF and Matrix similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can now create a `tfIdfModel` from `gensim`. The tfidfModel score will automatically give weigths to each of the keywords in the corpus. Based on these scores we will calculate the similarities between different movies using `MatrixSimilarity` function from `gensim`.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7f\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf-idf model for the corpus created above \n",
    "tfidf = ...\n",
    "\n",
    "# Create the similarity data structure. \n",
    "# This is the most important part where we get the similarities between the movies.\n",
    "# Hint: use the length of the dictionary as the num_features\n",
    "sims = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q7f\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "###  Part 4 : Putting it all together.  Movie Recommendation\n",
    "Finally, you have come to the point where you can put together a movie recommendation system. \n",
    "\n",
    "Pushkar has recently heard about _Batman_ and asks you what movies he should watch that are similar to it. You fire up your movie recommender and give him the best set of suggestions. \n",
    "\n",
    "The recommender would go through the following set of steps:\n",
    "\n",
    "1. Create the bag of words for the movie \n",
    "2. Calculate the tf-idf of the queried movie \n",
    "3. Calculate the similarity of the matrix\n",
    "4. See which movies are the most similar to the given movie \n",
    "5. Sort the current movies by the similarity score\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7g\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_recommendation(movie_title, dictionary, number_of_hits=10):\n",
    "    top_words = 5\n",
    "    # We will first start by  getting all the keywords related to the movies\n",
    "    movie = movies.loc[movies.primaryTitle==movie_title] # get the movie row\n",
    "    keywords = movie['key_words']# Get all the keywords\n",
    "    doc=[]\n",
    "    for word in keywords:\n",
    "        doc.extend(word)\n",
    "    \n",
    "    # Convert the doc into it's equivalent bag of words\n",
    "    query_doc_bow = ...\n",
    "    \n",
    "    # convert the regular bag of words model to a tf-idf model where we have tuples\n",
    "    # of the movie ID and its tf-idf value for the movie\n",
    "    query_doc_tfidf = tfidf[query_doc_bow]\n",
    "    \n",
    "    # get the array of similarity values between our movie and every other movie. \n",
    "    # To do this, we pass our list of tf-idf tuples to sims.\n",
    "    similarity_array = sims[query_doc_tfidf] \n",
    "    # the length is the number of movies we have. \n",
    "\n",
    "    similarity_series = pd.Series(similarity_array.tolist(), index=movies.primaryTitle.values) #Convert to a Series\n",
    "    top_hits = similarity_series.sort_values(ascending=False)[1:number_of_hits+1] \n",
    "    #get the top matching results, i.e. most similar movies; \n",
    "    # start from index 1 because every movie is most similar to itself\n",
    "\n",
    "    #print the words with the highest tf-idf values for the provided movie:\n",
    "    sorted_tfidf_weights = sorted(tfidf[corpus[movie.index.values.tolist()[0]]], key=lambda w: w[1], reverse=True)\n",
    "    print('Top %s words associated with this movie by tf-idf are: ' % top_words)\n",
    "    for term_id, weight in sorted_tfidf_weights[:top_words]:\n",
    "        print(\" '%s' (tf-idf score = %.3f)\" %(dictionary.get(term_id), weight))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Print the top matching movies\n",
    "    print(\"Top %s most similar movies for movie %s are:\" %(number_of_hits, movie_title))\n",
    "    top_movies=[]\n",
    "    for idx, (movie,score) in enumerate(zip(top_hits.index, top_hits)):\n",
    "        print(\"%d %s (similarity score = %.3f)\" %(idx+1, movie, score))\n",
    "        top_movies.append(movie)\n",
    "    return top_movies\n",
    "\n",
    "top_10_movies = movie_recommendation(\"Batman\", dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q7g\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminator = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Looking at the scores for the movies, how do the top recommendations for the movie _Inception_ differ from those for the movie _The Terminator_?** \n",
    "* If you have seen any of the movies or looked at the recommendations for other movies, comment on how well the system is able to recommend similar movies. \n",
    "* What else could we do to improve our movie recommendations?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7h\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have fun exploring the new movies whose plot's keywords are similar to other movies' plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**\n",
    "\n",
    "<!-- EXPECT 6 EXPORTED QUESTIONS -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to submit.\n",
    "import jassign.to_pdf\n",
    "jassign.to_pdf.generate_pdf('hw3.ipynb', 'hw3.pdf')\n",
    "ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
