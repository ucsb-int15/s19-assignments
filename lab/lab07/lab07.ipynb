{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab07.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 5: Regular Expressions, Text Processing\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the homework, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others please **include their names** at the top\n",
    "of your solution.\n",
    "\n",
    "\n",
    "## Due Date\n",
    "\n",
    "This assignment is due at **11:59pm Wednesday, May 22**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write names in this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "objectives",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Objectives for Lab 5:\n",
    "\n",
    "This lab has two main parts. \n",
    "\n",
    "In the first part, you will practice the basic usage of regular expressions and also learn to use `re` module in Python.  Some of the materials are based on the tutorial at http://opim.wharton.upenn.edu/~sok/idtresources/python/regex.pdf. As you work through the first part of the lab, you may also find the website http://regex101.com helpful. \n",
    "\n",
    "In the second part of the lab, we are going to practice NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Part 1: Regular Expressions\n",
    "\n",
    "We'll start by learning about the simplest possible regular expressions. Since regular expressions are used\n",
    "to operate on strings, we'll start with the most common task: matching characters.\n",
    "\n",
    "Most letters and characters will simply match themselves. For example, the regular expression `r'test'` will match the string `test` exactly. There are exceptions to this rule; some characters are special, and don't match themselves.\n",
    "\n",
    "Here is a list of metacharacters that are widely used in regular experssion. \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\" >\n",
    "<thead>\n",
    "  <tr style=\"text-align: right;\">\n",
    "    <th>Pattern </th>\n",
    "    <th>Description</th> \n",
    "  </tr>\n",
    " </thead>\n",
    " <tbody>\n",
    "  <tr>\n",
    "    <td>^</td>\n",
    "    <td>Matches beginning of line.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$</td>\n",
    "    <td>Matches end of line.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>.</td>\n",
    "    <td>Matches any single character except newline. </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>*</td>\n",
    "    <td>Matches 0 or more occurrences of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>+</td>\n",
    "    <td>Matches 1 or more occurrence of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>?</td>\n",
    "    <td>Matches 0 or 1 occurrence of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>[...]</td>\n",
    "    <td>Matches any single character in brackets.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>[^...]</td>\n",
    "    <td>Matches any single character <b>not</b> in brackets.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>{n}</td>\n",
    "    <td>Matches exactly n number of occurrences of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>{n,}</td>\n",
    "    <td>Matches n or more occurrences of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>{n,m}</td>\n",
    "    <td>Matches at least n and at most m occurrences of preceding expression.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>a|b</td>\n",
    "    <td>Matches either a or b.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\1...\\9</td>\n",
    "    <td>Matches n-th grouped subexpression.</td>\n",
    "  </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Perhaps the most important metacharacter is the backslash, `\\`. As in Python string literals, the backslash\n",
    "can be followed by various characters to signal various special sequences. It's also used to escape all the\n",
    "metacharacters so you can still match them in patterns; for example, if you need to match a `[` or `\\`, you\n",
    "can precede them with a backslash to remove their special meaning:  `\\[` or `\\\\`. \n",
    "\n",
    "The following predefined special sequences are available:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\" >\n",
    "<thead>\n",
    "  <tr style=\"text-align: right; font-size=14;\">\n",
    "    <th>Pattern </th>\n",
    "    <th>Description</th> \n",
    "  </tr>\n",
    " </thead>\n",
    " <tbody>\n",
    "  <tr>\n",
    "    <td>\\d</td>\n",
    "    <td>Matches any decimal digit; this is equivalent to the class `[0-9]`</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\D</td>\n",
    "    <td>Matches any non-digit character; this is equivalent to the class `[^0-9]`.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\s</td>\n",
    "    <td>Matches any whitespace character; this is equivalent to the class `[ \\t\\n\\r\\f\\v]` </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\S</td>\n",
    "    <td>Matches any non-whitespace character; this is equivalent to the class `[^ \\t\\n\\r\\f\\v]`.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\w</td>\n",
    "    <td>Matches any alphanumeric character; this is equivalent to the class `[a-zA-Z0-9_]`</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\\W</td>\n",
    "    <td>Matches any non-alphanumeric character; this is equivalent to the class `[^a-zA-Z0-9_]`.</td>\n",
    "  </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 1\n",
    "In this question, write patterns that match the given sequences. It may be as simple as the common letters on each line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Question 1a\n",
    "\n",
    "Write a single regular expression to match the following strings without using the `|` operator. Notice that the pattern must _start_ with \"abc\".\n",
    "\n",
    "1. **Match:** `abcdefg`\n",
    "1. **Match:** `abcde`\n",
    "1. **Match:** `abc`\n",
    "1. **Skip:** `c abc`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "regx1 = r\"\" # fill in your pattern\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Question 1b\n",
    "\n",
    "Write a single regular expression to match the following strings without using the `|` operator.\n",
    "\n",
    "1. **Match:** `can`\n",
    "1. **Match:** `man`\n",
    "1. **Match:** `fan`\n",
    "1. **Skip:** `dan`\n",
    "1. **Skip:** `ran`\n",
    "1. **Skip:** `pan`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "regx2 = r\"\" # fill in your pattern\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 2\n",
    "\n",
    "Now that we have written a few regular expressions, we are now ready to move beyond matching. In this question, we'll take a look at some methods from the `re` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Question 2a:\n",
    "\n",
    "Write a Python program to extract and print the numbers of a given string. \n",
    "\n",
    "1. **Hint:** use `re.findall`\n",
    "2. **Hint:** use `\\d` for digits and one of either `*` or `+`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "text_q2a = \"Ten 10, Twenty 20, Thirty 30\"\n",
    "\n",
    "res_q2a = ...\n",
    "...\n",
    "\n",
    "res_q2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Question 2b:\n",
    "\n",
    "Write a Python program to replace at most 2 occurrences of space, comma, or dot with a colon.\n",
    "\n",
    "**Hint:** use `re.sub(regex, \"newtext\", string, number_of_occurences)`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "text_q2b = 'Python Exercises, PHP exercises.'\n",
    "res_q2b = ... # Hint: use re.sub()\n",
    "...\n",
    "\n",
    "res_q2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Question 2c: \n",
    "\n",
    "Write a Python program to extract values between quotation marks of a string.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "text_q2c = '\"Python\", \"PHP\", \"Java\"'\n",
    "res_q2c = ... # Hint: use re.findall()\n",
    "...\n",
    "\n",
    "res_q2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2d:\n",
    "\n",
    "Write a regular expression to extract and print the quantity and type of objects in a string. You may assume that a space separates quantity and type, i.e., `\"{quantity} {type}\"`. See the example string below for more detail.\n",
    "\n",
    "1. **Hint:** use `re.findall`\n",
    "2. **Hint:** use `\\d` for digits and one of either `*` or `+`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2d\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_q2d = \"I've got 10 eggs that I stole from 20 gooses belonging to 30 giants.\"\n",
    "\n",
    "res_q2d = ...\n",
    "...\n",
    "\n",
    "res_q2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2e (optional):\n",
    "\n",
    "Write a regular expression to replace all words that are not `\"mushroom\"` with `\"badger\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qe = 'this is a word mushroom mushroom'\n",
    "res_qe = ... # Hint: https://www.regextester.com/94017\n",
    "...\n",
    "res_qe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2f (extra credit):\n",
    "\n",
    "Write a regular expression to replace all words that are `\"US\"` and `\"U.S.\"` with `\"USA\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qe = 'This will replace US and U.S. but not USGS with USA.'\n",
    "res_qe = ... \n",
    "...\n",
    "res_qe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Part 2: NLP\n",
    "\n",
    "Let's reproduce the example of extracting named entities from this article: [Discovering the essential tools for Named Entities Recognition](https://towardsdatascience.com/discovering-the-essential-tools-for-named-entities-recognition-8176c94d9747)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK module\n",
    "import nltk\n",
    "\n",
    "# Import word_tokenize \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import POS tagger\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload content from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup # a library for processing webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send a request to the website\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Natural_Language_Toolkit\")\n",
    "\n",
    "# Use BeautifulSoup to parse HTML using html5 protocol. It is slower\n",
    "# but more efficient \n",
    "page_content = BeautifulSoup(page.text) #, \"html5lib\")\n",
    "# page_content   # html source of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we look for the paragraphs\n",
    "textContent = []\n",
    "for i in range(0, 3):\n",
    "    paragraphs = page_content.find_all(\"p\")[i].text  # find the text inside the paragraph tag <p>\n",
    "    textContent.append(paragraphs)\n",
    "\n",
    "# Join the paragraphs together and replace the `\\n` for empty strings\n",
    "page_text = \" \".join(textContent).replace(\"\\n\", \"\")\n",
    "page_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag and tokenize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Create a method that takes the text as an input and use `nltk.word_tokenize` to split the text into tokens. Then, tag each token with its part of speech using `nltk.pos_tag`.\n",
    "\n",
    "The method will return a list of tuples, each consisting of a word along with its tag; the part of the speech that it corresponds to.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    This function takes a text. Split it in tokens using word_tokenize. \n",
    "    And then tags them using pos_tag from NLTK module.\n",
    "    It outputs a list of tuples. Each tuple consists of a word and the tag with its \n",
    "    part of speech.\n",
    "    \"\"\"\n",
    "    # Get the tokens\n",
    "    tokens = ...\n",
    "    # Tags the tokens\n",
    "    tagged_tokens = ...\n",
    "    # Returns the list of tuples\n",
    "    return tagged_tokens\n",
    "\n",
    "# Split and label the text\n",
    "label_text = preprocess_text(page_text)\n",
    "\n",
    "# Print first 20 tuples, 5 per line\n",
    "#for i in range(0, 20, 5):\n",
    "#    print(label_text[i], label_text[i+1], label_text[i+2], label_text[i+3], label_text[i+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2_1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk the text to get named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now perform entity detection using a technique called **chunking**.\n",
    "\n",
    "Tokenization extracts only “tokens” or words, whereas, chunking extracts phrases that may have an actual meaning in the text.\n",
    "\n",
    "Chunking requires that our text is first tokenized and POS tagged. It uses these tags as inputs. It outputs “chunks” that can indicate entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can first apply noun pronoun chunks or NP-chunks. We’ll look for chunks matching individual noun phrases. For this, we will customize the regular expressions used in the mechanism.\n",
    "\n",
    "We first need to define rules. They will indicate how sentences should be chunked. You can define your own rules, if you want to extract different chunks.\n",
    "\n",
    "For reference, here's the [list of tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "Our rule states that our NP chunk should consist of an optional determiner (DT) followed by any number of adjectives (JJ) and then one or more pronoun noun (NNP).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rule\n",
    "rule = \"NP: {<DT>?<JJ>*<NNP>+}\" \n",
    "\n",
    "# We define the parser using the rule\n",
    "parser = nltk.RegexpParser(rule) \n",
    "\n",
    "# Apply to the tagged words \n",
    "# by using the parse() function of the parser you just created\n",
    "# and giving it the label_text\n",
    "result = ...\n",
    "\n",
    "# Print only the chunks\n",
    "for entity in result:\n",
    "    if type(entity) == nltk.tree.Tree:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2_2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can instead use a pre-trained classifier using the function `nltk.ne_chunk()`. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ne_chunk to get entities \n",
    "named_entities = ...\n",
    "\n",
    "# Print only those that are recognized as entities\n",
    "# Entities have type nltk.tree.Tree\n",
    "for entity in named_entities:\n",
    "    if type(entity) == nltk.tree.Tree:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2_3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the results are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) for topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now apply LDA to classify text in a document to a particular topic [[1]](https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925). LDA builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n",
    "\n",
    "* Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "* LDA assumes that the every chunk of text we feed into it will contain words that are somehow related. Therefore choosing the right corpus of data is crucial.\n",
    "* It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution.\n",
    "\n",
    "To learn more about LDA check out this [link](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another page and see whether LDA will be able to classify their text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send a request to the website\n",
    "page1 = requests.get(\"https://en.wikipedia.org/wiki/Shallow_parsing\")\n",
    "\n",
    "page1_content = BeautifulSoup(page1.text) #, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we look for the paragraphs\n",
    "text1Content = []\n",
    "for i in range(0, 3):\n",
    "    paragraphs = page1_content.find_all(\"p\")[i].text  # find the text inside the paragraph tag <p>\n",
    "    text1Content.append(paragraphs)\n",
    "\n",
    "# Join the paragraphs together and replace the `\\n` for empty strings\n",
    "page1_text = \" \".join(text1Content).replace(\"\\n\", \"\")\n",
    "page1_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim # another module for text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "stop_words = gensim.parsing.preprocessing.STOPWORDS\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [page_text, page1_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = ...\n",
    "\n",
    "    # remove stop words from tokens: use list comprehension\n",
    "    stopped_tokens = ...\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = ...\n",
    "    \n",
    "    # add tokens to list\n",
    "    #texts.append(stopped_tokens)\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Are stemmed tokens important?\n",
    "\n",
    "Run the analysis below using `stemmed_tokens`, then come back and comment-out the creation of the stemmed tokens in the code above (making sure to properly update `texts`). How does the result change? **Write down your observations and analysis.**\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_4\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the text to bag of words and run LDA\n",
    "\n",
    "For topic modelling, we need to convert the preprocessed text to a bag of words, which is a dictionary where the key is a word and the value is the number of times that word occurs in the entire corpus. \n",
    "\n",
    "We then run LDA on our corpus, after we specify how many topics are there in the data set and how many training passes to do over the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix (bag-of-words)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 0 \n",
    "bow_doc_x = corpus[document_num]\n",
    "\n",
    "num_items = len(bow_doc_x)\n",
    "num_items = 12\n",
    "for i in range(num_items):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We are ready to interpret the model. The output from the model is a list of topics each categorized by a series of words along with the weight of that word in that topic. If you had to come up with the names for the 3 topics that LDA identified, what would you call them?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_5\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "finish",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Congrats! You are finished with this assignment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**\n",
    "\n",
    "<!-- EXPECT 2 EXPORTED QUESTIONS -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to submit.\n",
    "import jassign.to_pdf\n",
    "jassign.to_pdf.generate_pdf('lab07.ipynb', 'lab07.pdf')\n",
    "ok.submit()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
